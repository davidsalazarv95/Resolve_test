---
title: 'Resolve: Prueba Data Scientist'
author: "David Salazar"
date: "11/01/2017"
output: 
  html_document:
    toc: true
---

# 0. Organización

En este documento doy respuesta a las cuatro preguntas en el mismo orden en que fueron
formualdas. Cada sección en adelante representará una de las respectivas respuestas. El código detrás de todas las figuras y números presentados está disponible en [este](https://github.com/davidsalazarv95/Resolve_test) repositorio de GitHub. A lo largo del documento, utilizo los nombres de las variables que me proporcionaron en el archivo con las preguntas. 

# 1. ¿Cuáles considera que son las variables que determinan el precio de la vivienda?

Para entender qué variables determinan el precio promedio de las viviendas en los barrios, voy a investigar lo siguiente: 1) la variación de los precios y la variación conjunta de los precios con las otras variables; 2) el poder predictivo de estas variaciones. 

Para responder esta pregunta utilizaré las siguientes herramientas. Primero, haré una exploración y
análisis de datos, principalmente de la variable de interés: **valor** y de su relación con las otras variables en el archivo. Segundo, utilizaré dos algoritmos (**regresión multivariada** y **árboles de decisión**) para determinar el valor explicativo de cada una de las variables sobre **valor**. Por último, concluiré la respuesta a esta pregunta. 

## Exploración y análisis de datos

Primero, leemos los datos y confirmamos que no haya datos repetidos ni *missing*. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
source("0-Funciones_Iniciales.R")
base_datos <- read_and_check()
```

Un análisis exploratorio nos permite ver la variación singular y conjunta de las variables de interés. Primero, estudiaré la variación singular de la variable **valor**. Segundo, analizaré la variación conjunta de los precios medios en los barrios y las otras variables presentes. 

### Análsis de variable valor

En esta sección, presento un análisis de la variable de interés **valor**. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("1-Exploratory_Data_Analysis.R")
histograma_valor(base_datos)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(base_datos$valor)
cat("La desviación estándar de valor es: ", round(sd(base_datos$valor), 1))
```


La variable **valor** tiene una mediana de $404.9$ y presenta una asimetría hacía la derecha, presentando un tercer quartil de $477.5$ pero un nivel máximo de $955$. Es decir, hay barrios donde el valor medio de las casas es mucho mayor (más de dos desviaciones estándar superior al tercer cuartil) al del resto de los barrios presentes en la base de datos. 

### Análisis de variación conjunta 

Para analizar la variación conjunta, primero, estudiaré la relación lineal entre todas las variables presentes en la base de datos; segundo, estudiaré por medio de diagramas de dispersión la relación entre la variable **valor** y las variables que están más correlacionadas con esta. 

#### Matriz de correlaciones

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
matriz_correlaciones(base_datos)
```

Hay que resaltar dos cosas de la matriz de correlaciones: 

1. Hay un cluster de variables que están altamente correlacionadas entre sí y a su vez todas están negativamente correlacionadas con el precio medio de los inmuebles en el barrio. Tales como: **tasaeducativa, crimen, pobreza, movilidad, impuestos, industrial, oxidonitroso**. De este resultado, puedo concluir lo siguiente: hay posibilidad de reducir dimensionalidad sin perder poder explicativo, y esta correlación interna al cluster puede generar problemas de multicolinealidad en posibles regresiones. 

2. Pocas variables están altamente y positivamente correlacionadas con la variable **valor**. Solo la variable **cuartos** parece resaltar. 

Las tres variables que presentan una correlación más fuerte con **valor** son:

```{r echo=FALSE, message=FALSE, warning=FALSE}
big_three(base_datos)
```

Es decir, a mayor proporción de personas clasificadas como pobres, y mayor número de estudiantes por profesor, menor tiende a ser el valor promedio de los inmuebles en el barrios. Y, por otro lado, a mayor número de cuartos promedio tiene el barrio, mayor tiende a ser el valor promedio de los inmuebles en el barrio. Esto lo podemos ver explícitamente al graficar para cada barrio el **valor** y su correspondiente **pobreza, cuartos, y tasaeducativa**.

```{r echo=FALSE, message=FALSE, warning=FALSE}
scatterplots(base_datos)
```

### Ubicación

En el análisis anterior, se pudo establecer la relación líneal entre cada variable y la variable **valor**. Sin embargo, podemos decir que para el caso de la ubicación (**coordy, coordx**), lo que importa es la interacción entre estas para predecir **valor**. Si esto es correcto, habrá clusters de barrios con valores medios de inmuebles muy similares. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
ubicación(base_datos)
```

Exceptuando un cluster en el centro de la ciudad con valores medios inferiores a la media, no hay una tendencia general que muestre clusters de barrios. Luego, no parece que la interacción entre estas variables de coordenadas ofrezcan riqueza de información. 

### Conclusión

Gracias a la exploración de análisis de datos, podemos concluir lo siguiente:

1. Los precios medios de inmuebles en el barrio tienen una distribución con una cola derecha más larga que la izquierda. Es decir, hay barrios con precios mucho superiores a la media de la distribución. 

2. Los precios medios de los inmuebles en los barrios tienen relaciones líneales negativas con **tasaeducativa, crimen, pobreza, movilidad, impuestos, industrial, oxidonitroso**. Exceptuando el número de cuartos promedio, ninguna variable tiene correlaciones positivas de resaltar con el precio medio de los inmuebles en los barrios. 

## Valor explicativo de las variables

El ejercicio anterior no tuvo en cuenta tres características necesarias para definir qué variables determinan la variable **valor**. **Primero**, no tuvo en cuenta la variación simultánea de las variables. Es decir, puede que el porcentaje del barrio construido industrialmente solo esté relacionado con **valor** negativamente porque a mayor porcentaje del barrio construido industrialmente hay pobreza. Luego, al ver la variación simultánea de pobreza y porcentaje industrial, podríamos concluir. **Segundo**, vimos variación conjunta pero no tuvimos en cuenta la significancia estadística de estas relaciones. **Tercero**, no tuvimos en cuenta el poder predictivo de estas variaciones conjuntas. El propósito de esta sección es verificar estas tres características y así poder definir qué variables son las más importantes al explicar el precio medio de los inmuebles en un barrio. 

Para remediar los dos primeros puntos, construiré una regresión lineal con la variable valor como variable dependiente. Es decir, estimaré la siguiente ecuación utilizando la totalidad de la muestra: 

$$ valor_i = \alpha + \beta_1 * crimen_i + \beta_2 * residencial + \beta_3 * industrial_i + \beta_4 * rio_i +
 \beta_5 * oxidonitroso_i + \beta_6 * cuartos_i + \beta_7 * distanciaempleo_i \\ + beta_8 *movilidad_i + beta_9 *tasaeducativa_i + \beta_{10} * pobreza + \beta_{11} * coordx + beta_{12} * coordy $$
 
Para remediar el tercer punto, estimaré un modelo de árboles de regresión que nos ayude a ver la importancia relativa de cada variable para predecir el precio medio de los inmuebles en un barrio.  

### Regresión

Voy a estimar la regresión antes referida, pero con la salvedad que voy a estandarizar las variables continuas tal que tengan media $0$ y desviación estándar de $1$. Así, al comparar los coeficientes, la interpretación será la misma para todas las variables continuas: dado un aumento de una desviación en la variable $i$, ceteris paribus, qué tanto predecimos que aumentará la variable **valor**


```{r echo=FALSE, message=FALSE, warning=FALSE}
source("2-Valor_Explicativo.R")
regresion(base_datos)
```

La interpretación del gráfico es la siguiente: el punto es el estimado del coeficiente respectivo, mientras que la barra cubre la totalidad del intervalo de confianza. Si el intervalo de confianza cubre cero, el estimado no es estadísticamente significativo. 

Con respeccto a nuestros análisis anteriores, tanto **pobreza, tasaeducativa y cuartos** siguen teniendo efectos estimados altos (de alrededor de cambios en $50 por cada desviación estándar de cada variable, teniendo en cuenta que las primeras dos tienen un efecto negativo y la última un efecto positivo) sobre el precio medio de los inmuebles en los barrios. Luego, este ejercicio confirma que estas variables determinan en gran parte la variable **valor**. 

Por otro lado, las variables que no habíamos incluido antes en el análisis, están estimadas de manera muy imprecisa. Es decir, aunque sus coeficientes estimados pueden ser altos, sus intervalos de confianza son muy amplios, aún cuando no cubren el cero. Por esto, seguiré sin incluirlas como variables determinantes. 

Por último, vimos en la matriz de correlación que muchas de las variables estaban altamente correlacionadas entre sí. Esto puede generar multicolinealidad y que estimemos imprecisamente las variables. Una posible solución en un trabajo posterior sería realizar primero **Principal Component Analysis** y correr una regresión sobre los componentes obtenidos. 

### Árboles de regresión


```{r echo=FALSE, message=FALSE, warning=FALSE}
random_forest_importance(base_datos)
```


El gráfico anterior muestra el poder predictivo de cada variable dentro del algoritmo de **Random Forest** que estimamos para toda los datos. En ellos, las variables que mayor poder predictivo tienen siguen siendo: **cuartos y pobreza**. 

## Conclusión

Después de realizar un análisis exploratorio de los datos, un análisis de regresión, y un análisis del valor predictivo de las variables, dos variables resaltaron siempre a través del ejercicio: **cuartos y pobreza**. Por un lado, **pobreza** es una variable que está correlacionada con todas las otras variables que determinan la calidad de vida en un barrio (contaminación, tasa educativa, crimen, et cetera) y, por otro, **cuartos** parece recoger el efecto de muchas variables que determinan la calidad y tamaño de los edificios, variables que no están presentes en los datos. 

Luego, el valor medio de los inmuebles en el barrio está determinado por dos tipos de variables. Primero, una que determina la calidad de vida dentro del barrio. En este caso, **pobreza** tiene suficiente riqueza de información y es la más importante dentro de este tipo de variables. Segundo, la calidad del inmueble a comprar dentro del barrio, que en este caso es recogida en los datos por la variable **cuartos**.

Por lo tanto, concluyo que las variables que principalmente determinan el precio medio de los inmuebles dentro de un barrio son: **la proporción de personas clasificadas como pobres y el número de cuartos promedio**.

# 2. Formule hipótesis sobre cómo las variables descritas en el punto anterior pueden incidir en el precio de los inmuebles, yendo más allá de los resultados matemáticos.

Cualquier hipótesis que podamos formular, y posteriormente aprender de los datos, tendrá que seguir la lección básica del aprendizaje estadístico para poder generalizar y predecir en datos de los cuales no ha aprendido directamente. Esta lección es el *trade-off* entre el sesgo y la varianza en las hipótesis que formulamos. Si tenemos un espacio de posibles hipótesis muy grande, donde podemos aproximar cualquier función, posiblemente reduciremos el sesgo pero estaremos corriendo el riesgo de aumentar la varianza. Sin embargo, si tenemos un espacio de posibles hipótesis muy reducido y con poca varianza, corremos el riesgo de aumentar el sesgo.

El factor que determina en qué posición del trade-off podemos estar es la cantidad y riqueza de datos que tenemos. Debido a que en este caso tengo a dispoción alrededor de *500* datos, voy a utilizar primero unos algoritmos con poca varianza, como es *ridge regression, lasso regression y regresión*, y posteriormente utilizare UN algoritmo con mayor varianza, como *gradient boosting trees*. En todos los casos, voy a utilizar una división simple de los datos para el aprendizaje, la validación y la estimación del error generalizable. Respectivamente, una división de datos en *train set, validation y test set*.  Mediante *cross-validation con el train set* compararé los errores por fuera de muestra de todos los algoritmos y sus respectivas combinaciones y escogeré uno de ellos, y estimare este modelo en todo el *train set*. En el *validation set*, lo utilizaré para escoger entre los modelos. Con el *test set*, estimaré el error de generalización para el algortimo escogido en el último paso. 

En todos los pasos, la medida de evaluación que utilizaré será el *Mean Square Error*, dado que estamos en el contexto de un problema de regresión. 

```{r message=FALSE, warning=FALSE, include=FALSE}
source("3-Hipotesis.R")
set.seed(2)
datasets <- split_train_validation_test(base_datos)
```

## Algoritmos poca varianza

```{r echo=FALSE, message=FALSE, warning=FALSE}
ridge <- ridge_regression(datasets)
lasso <- lasso_regression(datasets)
reg <- regresion_simple(datasets)
```

*Ridge regression* es una variación de una regresión simple, donde se penaliza el tamaño (suma al cuadrado de los coeficientes) de los coeficientes en el proceso de optimización. Mientras que en *lasso*, se penaliza por la suma de los valores absolutos de los coeficientes. Luego, con estos tipos de regularización, normalmente se consigue una mejor generalización que con una *regresión simple*. A continuación, muestro los errores en el validation set para cada uno de los tres tipos de regresión. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
rbind(ridge[[2]], reg[[2]], lasso[[2]]) %>% 
  rename(validation_error = mse, type_of_regression = name) %>% 
  arrange(validation_error)
```

En conclusión, ninguna de los tipos de regularización funcionó, pues la regresión simple fue la que mejor resultado. Por lo tanto, podemos concluir que hay espacio para utilizar algoritmos que consideren espacios de hipótesis más complejos y podamos mejorar el error de validación. 

## Algoritmos mayor varianza: Gradient Boosting Trees

```{r echo=FALSE, message=FALSE, warning=FALSE}
gbm_results <- gbm_over(datasets, base_datos)
gbm_results[[2]]
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
gbm_results[[2]]$hyperparameter
```

En comparación a los modelos de regresión, el espacio de hipótesis de este algoritmo es muchísimo más amplio. Uno de los problemas de trabajar con este tipo de modelos es la posibilidad de *overfitting*. Problema que tiene este algoritmo: la diferencia entre el error de validación y el error en el training set es muy grande. Luego, el modelo está aprendiendo parte del "ruido" que hay en el training set; como este "ruido" no está presente en el *validation set*, los resultados de predicción empeoran.

Aún así, el algoritmo predice mejor los valores medios de los inmuebles en los barrios para el *validation set* que cualquiera de los otros algoritmos de regresión utilizados. 

Luego, la existencia de un algoritmo más apropiado para el problema, pero aún con problemas de *overfitting*, abre la posibilidad a seguir trabajando en un algoritmo con una complejidad intermedia y que mejore aún más los resultados al predecir. 

## Conclusión

De los cuatro algoritmos utilizados, el más complejo, *Gradient Boosting trees*, fue el que mejor resultados dio en el *validation set*. Sin embargo, el modelo sufre de *overfitting*. Por esto, existe la posibilidad de encontrar un mejor algoritmo con una complejidad intermedia y que mejore los resultados de predicción. 

Por último, este es el error del modelo en el *test set*:

```{r echo=FALSE, message=FALSE, warning=FALSE}
cat("El error en el test set es: " , gbm_results[[3]])
```

# 3. ¿Qué otras variables cree que podrían afectar el precio de la vivienda y que no están incluidas en el conjunto?

Las variables que más podrían ayudar a mejorar la predicción son las variables que describen el estado del inmueble en cuestión, ya que de este tipo solo está **cuarto**. Es decir, la mayoría de las variables se concentran en describir la calidad de vida en el barrio en la que están los inmuebles, pero no el tamaño, edad del inmueble, remodelaciones hechas, tipo de piso, tipo de construcción, # de baños, tipo de cocina, et cetera, promedio de los inmuebles en cuestion. 

Es por esto que la mayoría de las variables, que en general miden lo mismo (calidad de vida en barrio), están muy correlacionadas entre sí (ver Matriz correlaciones) y al final se pueden resumir por una variable: **pobreza** (ver importancia predictiva de las variables). Igualmente, es por esta razón que la variable **cuartos** tiene tanta importancia (ver importancia predictiva de las variables): es la única que mide algo distinto al resto de las variables. 

Una forma de confirmar esto matemáticamente es mediante *Principal Component Analysis*. Con este algoritmo, podemos ver las combinaciones de las variables tales que las observaciones varien lo máximo, y estas combinaciones sean independientes entre sí. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggfortify)
x <- as.matrix(base_datos %>% select(-c(valor, index)) %>% mutate_all(as.numeric))
pca <- prcomp(x, scale. = TRUE)
autoplot(pca, alpha = 0.3, loadings = TRUE, loadings.colour = 'red3',
         loadings.label = TRUE, loadings.label.size = 3) +
  theme_ipsum_rc() +
  labs(title = "Principal Components Loadings")
```


Este *biplot* se entiende de la siguiente forma: entre más horizontal la línea de cada variable, más importante es en el primer componente (primera combinación lineal), y entre más vertical, más importante en el segundo componente. De otro lado, entre más cerca las líneas entre sí, más correlacionadas están las respectivas líneas entre sí. 

Por lo tanto, se ve que la mayoría de las variables son horizontales, y se concentran en crear un primer componente que mide la calidad de vida al ponderar por **tasaeducativa, pobreza, crimen, industrial, oxidonitroso, impuestos, movilidad, distanciaempleo, residencial**, mientras el segundo componente mide otras características: como **cuartos, rio, coordx, coordy**. Es decir, una combinación del tamaño promedio de los inmuebles y su ubicación geográfica. Luego, vemos que hay mucho *overlap* en lo que miden las variables, y hay espacio para utilizar otro tipo de variables que midan lo que **cuarto** no logre capturar de las características de los inmuebles. 

## Conclusión

Intuitivamente, muchas de las variables en los datos miden una misma característica: la calidad de vida en el barrio medida por calidad de medio ambiente, educación, crimen, et cetera. Otras, miden la ubicación geográfica en la ciudad, como **río, coordy, coordx**. Sin embargo, solo **cuartos** mide el tamaño y calidad promedio de los inmuebles. Estas conclusiones las confirmé mediante un análisis de *PCA*. 

Luego, las variables que pueden afectar el precio promedio de los inmuebles y que no logran capturar los modelos son las variables que tienen que ver con los inmuebles en sí y que **cuartos** no logra capturar. Tales como: el tamaño, edad del inmueble, remodelaciones hechas, tipo de piso, tipo de construcción, # de baños, tipo de cocina, et cetera, promedio de los inmuebles en cuestion. 

# 4. Dada la información disponible, ¿cuales considera que son observaciones atípicas?

```{r echo=FALSE, message=FALSE, warning=FALSE}
gbm_results[[4]] %>% 
  ggplot(aes(x = y_total, y = y_todos)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1) +
    theme_ipsum_rc() +
  scale_y_continuous(limits = c(0, 1000)) +
  scale_x_continuous(limits = c(0, 1000)) +
  labs(x = "Valor", y = "Valor Predicho",
       title = "Valor vs. Valor Predicho",
       subtitle = "Línea: y = x")
```




